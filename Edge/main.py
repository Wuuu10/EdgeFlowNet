import os
import sys
import argparse
import json
import torch

from config import Config
from train import LightweightTrainer
from validate import validate_model, validate_multi_models, LightweightModelValidator
from ablation import LightweightAblationStudy


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='è½»é‡çº§è¾¹ç¼˜æ£€æµ‹ç½‘ç»œ - ä¸»ç¨‹åº',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # è®­ç»ƒæ¨¡å‹
  python main.py --mode train --experiment_name my_experiment

  # éªŒè¯æ¨¡å‹
  python main.py --mode validate --checkpoint ./checkpoints/best_model.pth

  # è¿è¡Œæ¶ˆèç ”ç©¶
  python main.py --mode ablation --ablation_mode quick

  # æ€§èƒ½åŸºå‡†æµ‹è¯•
  python main.py --mode benchmark --checkpoint ./checkpoints/best_model.pth

  # å¤šæ¨¡å‹æ¯”è¾ƒ
  python main.py --mode compare --checkpoint ./checkpoints/ --visualize
        """
    )

    # ä¸»è¦å‚æ•°
    parser.add_argument('--mode', type=str, default='train',
                        choices=['train', 'validate', 'test', 'ablation', 'benchmark', 'compare', 'profile'],
                        help='è¿è¡Œæ¨¡å¼')

    parser.add_argument('--config', type=str, default=None,
                        help='è‡ªå®šä¹‰é…ç½®æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)')

    parser.add_argument('--checkpoint', type=str, default=None,
                        help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ (æ–‡ä»¶æˆ–ç›®å½•)')

    parser.add_argument('--experiment_name', type=str, default=None,
                        help='å®éªŒåç§°')

    # éªŒè¯å’Œå¯è§†åŒ–å‚æ•°
    parser.add_argument('--visualize', action='store_true',
                        help='ç”Ÿæˆå¯è§†åŒ–ç»“æœ')

    parser.add_argument('--save_reports', action='store_true',
                        help='ä¿å­˜è¯¦ç»†æŠ¥å‘Š')

    parser.add_argument('--evaluate_edges', action='store_true',
                        help='è¯„ä¼°è¾¹ç¼˜åŒºåŸŸæ€§èƒ½')

    # æ¶ˆèç ”ç©¶å‚æ•°
    parser.add_argument('--ablation_mode', type=str, default='quick',
                        choices=['quick', 'full', 'channel', 'module', 'edge'],
                        help='æ¶ˆèç ”ç©¶æ¨¡å¼')

    parser.add_argument('--max_epochs', type=int, default=50,
                        help='æ¶ˆèç ”ç©¶çš„æœ€å¤§è®­ç»ƒè½®æ•°')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--resume', type=str, default=None,
                        help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')

    parser.add_argument('--efficient_mode', action='store_true',
                        help='ä½¿ç”¨æ•ˆç‡ä¼˜åŒ–ç‰ˆè®­ç»ƒå™¨')

    # è¾“å‡ºå‚æ•°
    parser.add_argument('--result_dir', type=str, default=None,
                        help='ç»“æœä¿å­˜ç›®å½•')

    parser.add_argument('--quiet', action='store_true',
                        help='å‡å°‘è¾“å‡ºä¿¡æ¯')

    return parser.parse_args()


def load_custom_config(config_path):
    """åŠ è½½è‡ªå®šä¹‰é…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = json.load(f)

    # åˆ›å»ºåŸºç¡€é…ç½®
    config = Config()

    # æ›´æ–°é…ç½®
    for key, value in config_dict.items():
        if hasattr(config, key):
            setattr(config, key, value)
        else:
            print(f"è­¦å‘Š: æœªçŸ¥é…ç½®é¡¹ {key}")

    print(f"å·²åŠ è½½è‡ªå®šä¹‰é…ç½®: {config_path}")
    return config


def train_mode(config, args):
    """è®­ç»ƒæ¨¡å¼"""
    print("è®­ç»ƒæ¨¡å¼")

    # é€‰æ‹©è®­ç»ƒå™¨
    if args.efficient_mode:
        trainer_class = LightweightTrainer
        print("ä½¿ç”¨æ ‡å‡†è®­ç»ƒå™¨")

    # åˆ›å»ºè®­ç»ƒå™¨
    experiment_name = args.experiment_name or config.EXPERIMENT_NAME
    trainer = trainer_class(config, experiment_name)

    # ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    start_epoch = 0
    if args.resume:
        if os.path.exists(args.resume):
            start_epoch = trainer.resume_training(args.resume)
            config.NUM_EPOCHS = config.NUM_EPOCHS - start_epoch
            print(f"ä»ç¬¬ {start_epoch} è½®æ¢å¤è®­ç»ƒ")
        else:
            print(f"è­¦å‘Š: æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨ {args.resume}")

    # å¼€å§‹è®­ç»ƒ
    best_miou = trainer.train()

    print(f"è®­ç»ƒå®Œæˆ")
    print(f"æœ€ä½³mIoU: {best_miou:.4f}")
    print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {os.path.join(config.MODEL_SAVE_DIR, experiment_name)}")

    return best_miou


def validate_mode(config, args):
    """éªŒè¯æ¨¡å¼"""
    print("å¯åŠ¨éªŒè¯æ¨¡å¼")

    if not args.checkpoint:
        raise ValueError("éªŒè¯æ¨¡å¼éœ€è¦æŒ‡å®š --checkpoint å‚æ•°")

    experiment_name = args.experiment_name or f"validation_{os.path.basename(args.checkpoint).split('.')[0]}"

    # è®¾ç½®è¾¹ç¼˜è¯„ä¼°
    config.EVALUATE_EDGES = args.evaluate_edges

    results = validate_model(
        config=config,
        checkpoint_path=args.checkpoint,
        result_dir=args.result_dir,
        visualize=args.visualize,
        experiment_name=experiment_name
    )

    print("éªŒè¯å®Œæˆ!")
    return results


def compare_mode(config, args):
    """æ¯”è¾ƒæ¨¡å¼"""
    print("å¯åŠ¨å¤šæ¨¡å‹æ¯”è¾ƒæ¨¡å¼")

    if not args.checkpoint:
        raise ValueError("æ¯”è¾ƒæ¨¡å¼éœ€è¦æŒ‡å®š --checkpoint å‚æ•°")

    if os.path.isdir(args.checkpoint):
        # ä»ç›®å½•ä¸­æ‰¾æ‰€æœ‰æ¨¡å‹
        checkpoint_paths = {}
        for filename in os.listdir(args.checkpoint):
            if filename.endswith('.pth'):
                name = os.path.splitext(filename)[0]
                checkpoint_paths[name] = os.path.join(args.checkpoint, filename)

        if not checkpoint_paths:
            raise ValueError(f"åœ¨ç›®å½• {args.checkpoint} ä¸­æœªæ‰¾åˆ° .pth æ–‡ä»¶")

        print(f"æ‰¾åˆ° {len(checkpoint_paths)} ä¸ªæ¨¡å‹æ–‡ä»¶")
        for name in checkpoint_paths:
            print(f"  - {name}")

    else:
        raise ValueError("æ¯”è¾ƒæ¨¡å¼éœ€è¦æŒ‡å®šåŒ…å«å¤šä¸ªæ¨¡å‹çš„ç›®å½•")

    # è®¾ç½®è¾¹ç¼˜è¯„ä¼°
    config.EVALUATE_EDGES = args.evaluate_edges

    results = validate_multi_models(
        config=config,
        checkpoint_paths=checkpoint_paths,
        result_dir=args.result_dir,
        visualize=args.visualize
    )

    print("æ¨¡å‹æ¯”è¾ƒå®Œæˆ")
    return results


def benchmark_mode(config, args):
    """åŸºå‡†æµ‹è¯•æ¨¡å¼"""
    print("âš¡ å¯åŠ¨æ€§èƒ½åŸºå‡†æµ‹è¯•æ¨¡å¼")

    if not args.checkpoint:
        raise ValueError("åŸºå‡†æµ‹è¯•æ¨¡å¼éœ€è¦æŒ‡å®š --checkpoint å‚æ•°")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"æµ‹è¯•è®¾å¤‡: {device}")

    # åˆ›å»ºéªŒè¯å™¨
    validator = LightweightModelValidator(config, args.checkpoint, device)

    print("\næ¨¡å‹ä¿¡æ¯:")
    model_info = validator.model_info
    print(f"  æ€»å‚æ•°é‡: {model_info['total_params']:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {model_info['trainable_params']:,}")
    print(f"  æ¨¡å‹å¤§å°: {model_info['model_size_mb']:.2f} MB")

    print("\né€Ÿåº¦åŸºå‡†æµ‹è¯•:")
    speed_results = validator.benchmark_inference_speed(num_runs=200)
    print(f"  å¹³å‡æ¨ç†æ—¶é—´: {speed_results['avg_inference_time_ms']:.2f} ms")
    print(f"  FPS: {speed_results['fps']:.1f}")
    print(f"  ååé‡: {speed_results['throughput_imgs_per_sec']:.1f} images/sec")

    print("\n å†…å­˜åŸºå‡†æµ‹è¯•:")
    memory_results = validator.measure_gpu_memory()
    if device.type == 'cuda':
        print(f"  GPUå†…å­˜å³°å€¼: {memory_results['gpu_memory_peak_mb']:.1f} MB")
        print(f"  GPUå†…å­˜å½“å‰: {memory_results['gpu_memory_current_mb']:.1f} MB")
    else:
        print(f"  CPUå†…å­˜: {memory_results['cpu_memory_mb']:.1f} MB")

    # ç»¼åˆæ€§èƒ½åˆ†æ
    print(f"\nç»¼åˆæ€§èƒ½åˆ†æ:")
    comprehensive_results = validator.get_comprehensive_profile()

    efficiency_metrics = ['params_efficiency', 'flops_efficiency', 'memory_efficiency', 'overall_efficiency']
    for metric in efficiency_metrics:
        if metric in comprehensive_results:
            print(f"  {metric.replace('_', ' ').title()}: {comprehensive_results[metric]:.2f}")

    # ä¿å­˜æŠ¥å‘Š
    if args.save_reports:
        result_dir = args.result_dir or config.RESULT_DIR
        experiment_name = args.experiment_name or "benchmark_results"
        save_dir = os.path.join(result_dir, experiment_name)
        os.makedirs(save_dir, exist_ok=True)

        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_results = {**model_info, **speed_results, **memory_results, **comprehensive_results}

        # ä¿å­˜JSONç»“æœ
        with open(os.path.join(save_dir, 'benchmark_results.json'), 'w') as f:
            json.dump(all_results, f, indent=4)

        # ç”Ÿæˆæ•ˆç‡æŠ¥å‘Š
        from utils.metrics import generate_efficiency_report
        generate_efficiency_report(
            experiment_name,
            all_results,
            os.path.join(save_dir, "efficiency_report.md")
        )

        print(f"\nåŸºå‡†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_dir}")

    print(" åŸºå‡†æµ‹è¯•å®Œæˆ")
    return comprehensive_results


def ablation_mode(config, args):
    print("å¯åŠ¨æ¶ˆèç ”ç©¶æ¨¡å¼")
    print(f"æ¶ˆèæ¨¡å¼: {args.ablation_mode}")
    print(f"æœ€å¤§è®­ç»ƒè½®æ•°: {args.max_epochs}")

    result_dir = args.result_dir or os.path.join(config.RESULT_DIR, "ablation_results")

    if args.ablation_mode == 'full':
        ablation = LightweightAblationStudy(config, result_dir)
        ablation.run_full_lightweight_study(args.max_epochs)
        results = ablation.results

    elif args.ablation_mode == 'channel':
        print("è¿è¡Œé€šé“é…ç½®æ¶ˆèç ”ç©¶...")
        ablation = LightweightAblationStudy(config, result_dir)
        ablation.run_lightweight_channel_ablation()
        results = ablation.results

    elif args.ablation_mode == 'module':
        print("è¿è¡Œæ¨¡å—æ¶ˆèç ”ç©¶...")
        ablation = LightweightAblationStudy(config, result_dir)
        ablation.run_lightweight_module_ablation()
        results = ablation.results

    elif args.ablation_mode == 'edge':
        print("è¿è¡Œè¾¹ç¼˜å¤„ç†æ¶ˆèç ”ç©¶...")
        ablation = LightweightAblationStudy(config, result_dir)
        ablation.run_edge_processing_ablation()
        results = ablation.results

    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¶ˆèæ¨¡å¼: {args.ablation_mode}")

    print("æ¶ˆèç ”ç©¶å®Œæˆ")
    return results


def profile_mode(config, args):
    print("å¯åŠ¨æ€§èƒ½åˆ†ææ¨¡å¼")
    if not args.checkpoint:
        raise ValueError("æ€§èƒ½åˆ†ææ¨¡å¼éœ€è¦æŒ‡å®š --checkpoint å‚æ•°")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    validator = LightweightModelValidator(config, args.checkpoint, device)
    profile_results = validator.get_comprehensive_profile()

    print("\næ€§èƒ½åˆ†æ:")
    print(f"æ¨¡å‹å‚æ•°: {profile_results['total_params']:,}")
    print(f"æ¨¡å‹å¤§å°: {profile_results['model_size_mb']:.2f} MB")
    print(f"è®¡ç®—é‡: {profile_results['gflops']:.2f} GFLOPs")
    print(f"æ¨ç†é€Ÿåº¦: {profile_results['fps']:.1f} FPS")
    print(f"GPUå†…å­˜: {profile_results.get('gpu_memory_peak_mb', 0):.1f} MB")
    print(f"å‚æ•°æ•ˆç‡: {profile_results['params_efficiency']:.2f}")
    print(f"ç»¼åˆæ•ˆç‡: {profile_results['overall_efficiency']:.2f}")

    # è½»é‡åŒ–è¯„ä¼°
    from utils.metrics import LightweightBenchmark
    benchmark = LightweightBenchmark(validator.model, device)
    efficiency_summary = benchmark.get_efficiency_summary()

    print(f"\nè½»é‡åŒ–è¯„ä¼°:")
    print(f"è½»é‡åŒ–åˆ†æ•°: {efficiency_summary['lightweight_score']:.1f}/100")
    print(f"  å‚æ•°åˆ†æ•°: {efficiency_summary['param_score']:.1f}/100")
    print(f"  è®¡ç®—åˆ†æ•°: {efficiency_summary['flops_score']:.1f}/100")
    print(f"  é€Ÿåº¦åˆ†æ•°: {efficiency_summary['speed_score']:.1f}/100")

    # ä¿å­˜è¯¦ç»†åˆ†æ
    if args.save_reports:
        result_dir = args.result_dir or config.RESULT_DIR
        experiment_name = args.experiment_name or "profile_results"
        save_dir = os.path.join(result_dir, experiment_name)
        os.makedirs(save_dir, exist_ok=True)

        # ä¿å­˜æ€§èƒ½åˆ†æç»“æœ
        all_results = {**profile_results, **efficiency_summary}
        with open(os.path.join(save_dir, 'profile_results.json'), 'w') as f:
            json.dump(all_results, f, indent=4)

        # ç”Ÿæˆå‚æ•°åˆ†å¸ƒå›¾
        try:
            from utils.visualization import create_parameter_distribution_plot
            create_parameter_distribution_plot(validator.model, save_dir, experiment_name)
        except Exception as e:
            print(f"å‚æ•°åˆ†å¸ƒå›¾ç”Ÿæˆå¤±è´¥: {e}")

        print(f"\næ€§èƒ½åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_dir}")

    print("æ€§èƒ½åˆ†æå®Œæˆ")
    return profile_results


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if not args.quiet:
        print(f"è¿è¡Œæ¨¡å¼: {args.mode}")
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"GPUè®¾å¤‡: {torch.cuda.get_device_name()}")

    # åŠ è½½é…ç½®
    if args.config:
        config = load_custom_config(args.config)
    else:
        config = Config()

    # æ ¹æ®å‚æ•°æ›´æ–°é…ç½®
    if args.result_dir:
        config.RESULT_DIR = args.result_dir

    # æ˜¾ç¤ºé…ç½®æ‘˜è¦
    if not args.quiet:
        print(f"\né…ç½®æ‘˜è¦:")
        print(f"  æ•°æ®æ ¹ç›®å½•: {config.DATA_ROOT}")
        print(f"  è¾“å…¥å°ºå¯¸: {config.INPUT_SIZE}")
        print(f"  æ‰¹æ¬¡å¤§å°: {config.BATCH_SIZE}")
        print(f"  ç»“æœç›®å½•: {config.RESULT_DIR}")
        print(
            f"  æ¨¡å‹é€šé“: HF={config.HIGH_FREQ_CHANNELS}, Sem={config.SEMANTIC_CHANNELS}, Fus={config.FUSION_CHANNELS}")

    try:
        # æ‰§è¡Œç›¸åº”æ¨¡å¼
        if args.mode == 'train':
            result = train_mode(config, args)

        elif args.mode == 'validate':
            result = validate_mode(config, args)

        elif args.mode == 'compare':
            result = compare_mode(config, args)

        elif args.mode == 'benchmark':
            result = benchmark_mode(config, args)

        elif args.mode == 'ablation':
            result = ablation_mode(config, args)

        elif args.mode == 'profile':
            result = profile_mode(config, args)

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¿è¡Œæ¨¡å¼: {args.mode}")

        if not args.quiet:
            print(f"\nğŸ‰ {args.mode.upper()} æ¨¡å¼æ‰§è¡Œå®Œæˆ!")

        return result

    except KeyboardInterrupt:
        return None

    except Exception as e:
        if not args.quiet:
            import traceback
            traceback.print_exc()
        return None


if __name__ == "__main__":
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['PYTHONPATH'] = os.path.dirname(os.path.abspath(__file__))

    # è¿è¡Œä¸»ç¨‹åº
    result = main()

    # é€€å‡ºç¨‹åº
    if result is not None:
        sys.exit(0)
    else:
        sys.exit(1)